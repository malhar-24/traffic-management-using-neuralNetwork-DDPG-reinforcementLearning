{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***DDPG STRUCTURE***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to train high epochs but still this model need to br finetune and is less accurate that the nn model in this same repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim,hidden_dim=256):\n",
    "      super(Actor, self).__init__()\n",
    "      self.network = nn.Sequential(\n",
    "          nn.Linear(state_dim, hidden_dim),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(hidden_dim, hidden_dim),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(hidden_dim, action_dim),\n",
    "          nn.Tanh()\n",
    "      )\n",
    "\n",
    "    def forward(self, state):\n",
    "      return self.network(state)\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=256):\n",
    "        super(Critic, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(state_dim + action_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "        )\n",
    "    def forward(self,state,action):\n",
    "        x = torch.cat([state,action], dim=1)\n",
    "        return self.network(x)\n",
    "\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "  def __init__(self,capacity):\n",
    "    self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "  def push(self,state,action,reward,next_state,done):\n",
    "    self.buffer.append((state,action,reward,next_state,done))\n",
    "\n",
    "  def sample(self,batch_size):\n",
    "    transitions = random.sample(self.buffer,batch_size)\n",
    "    state,action,reward,next_state,done=zip(*transitions)\n",
    "\n",
    "    return (\n",
    "        torch.FloatTensor(state),\n",
    "        torch.FloatTensor(action),\n",
    "        torch.FloatTensor(reward),\n",
    "        torch.FloatTensor(next_state),\n",
    "        torch.FloatTensor(done)\n",
    "    )\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.buffer)\n",
    "\n",
    "\n",
    "class DDPG:\n",
    "  def __init__(self,state_dim,action_dim,hidden_dim=28,buffer_size=1000000,batch_size=64,gamma=0.99,tau=0.5,actor_lr=1e-4,critic_lr=1e-3):\n",
    "\n",
    "    self.actor=Actor(state_dim,action_dim,hidden_dim)\n",
    "    self.actor_target=Actor(state_dim,action_dim,hidden_dim)\n",
    "    self.actor_target.load_state_dict(self.actor.state_dict())\n",
    "\n",
    "    self.critic=Critic(state_dim,action_dim,hidden_dim)\n",
    "    self.critic_target=Critic(state_dim,action_dim,hidden_dim)\n",
    "    self.critic_target.load_state_dict(self.critic.state_dict())\n",
    "\n",
    "    self.actor_optimizer=optim.Adam(self.actor.parameters(),lr=actor_lr)\n",
    "    self.critic_optimizer=optim.Adam(self.critic.parameters(),lr=critic_lr)\n",
    "\n",
    "    self.replay_buffer=ReplayBuffer(buffer_size)\n",
    "\n",
    "    self.batch_size = batch_size\n",
    "    self.gamma=gamma\n",
    "    self.tau=tau\n",
    "\n",
    "\n",
    "  def select_action(self, state, noise_std=50):\n",
    "    with torch.no_grad():\n",
    "        state = torch.FloatTensor(state).unsqueeze(0)\n",
    "        action = self.actor(state).squeeze(0).numpy()  # Output is in [-1,1]\n",
    "\n",
    "    # Scale action from [-1, 1] to [1, 30]\n",
    "    min_action, max_action = 0, 30\n",
    "    action = (action + 1) / 2 * (max_action - min_action) + min_action  # Scale to [1, 30]\n",
    "\n",
    "    # Add noise\n",
    "    action += np.random.normal(0, noise_std, size=action.shape)\n",
    "\n",
    "    # Ensure within bounds, round, and convert to integer array\n",
    "    action = np.clip(action, min_action, max_action)  # Keep within [1, 30]\n",
    "    action = np.round(action).astype(int)  # Convert all elements to integers\n",
    "\n",
    "    return action  # Keep as NumPy array\n",
    "\n",
    "  def select_action1(self, state, noise_std=0.1):\n",
    "    with torch.no_grad():\n",
    "        state = torch.FloatTensor(state).unsqueeze(0)\n",
    "        action = self.actor(state).squeeze(0).numpy()  # Output is in [-1,1]\n",
    "\n",
    "    # Scale action from [-1, 1] to [1, 30]\n",
    "    min_action, max_action = 0, 30\n",
    "    action = (action + 1) / 2 * (max_action - min_action) + min_action  # Scale to [1, 30]\n",
    "\n",
    "    # Ensure within bounds, round, and convert to integer array\n",
    "    action = np.clip(action, min_action, max_action)  # Keep within [1, 30]\n",
    "    action = np.round(action).astype(int)  # Convert all elements to integers\n",
    "\n",
    "    return action  # Keep as NumPy array\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  def train(self):\n",
    "    if len(self.replay_buffer) < self.batch_size:\n",
    "      return\n",
    "\n",
    "    state,action,reward,next_state,done=self.replay_buffer.sample(self.batch_size)\n",
    "\n",
    "    with torch.no_grad():\n",
    "      next_action=self.actor_target(next_state)\n",
    "      target_Q=self.critic_target(next_state,next_action)\n",
    "      target_Q=reward.unsqueeze(1)+(1-done.unsqueeze(1))*self.gamma*target_Q\n",
    "\n",
    "    current_Q=self.critic(state,action)\n",
    "    critic_loss=nn.MSELoss()(current_Q,target_Q)\n",
    "\n",
    "    self.critic_optimizer.zero_grad()\n",
    "    critic_loss.backward()\n",
    "    self.critic_optimizer.step()\n",
    "\n",
    "    actor_loss = -self.critic(state,self.actor(state)).mean()\n",
    "\n",
    "\n",
    "    self.actor_optimizer.zero_grad()\n",
    "    actor_loss.backward()\n",
    "    self.actor_optimizer.step()\n",
    "\n",
    "\n",
    "    self._soft_update(self.actor_target, self.actor)\n",
    "    self._soft_update(self.critic_target, self.critic)\n",
    "\n",
    "  def _soft_update(self, target, source):\n",
    "\n",
    "    for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "        target_param.data.copy_(self.tau * param.data + (1.0 - self.tau) * target_param.data)\n",
    "\n",
    "  def save_model(self, filename=\"ddpg_agent.pth\"):\n",
    "        torch.save({\n",
    "            'actor_state_dict': self.actor.state_dict(),\n",
    "            'critic_state_dict': self.critic.state_dict(),\n",
    "            'actor_optimizer': self.actor_optimizer.state_dict(),\n",
    "            'critic_optimizer': self.critic_optimizer.state_dict(),\n",
    "        }, filename)\n",
    "        print(f\"Model saved to {filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test the model if:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_ddpg():\n",
    "    state_dim = 10\n",
    "    action_dim = 1\n",
    "    agent = DDPG(state_dim, action_dim)\n",
    "\n",
    "    state = np.random.randn(state_dim)\n",
    "    action = agent.select_action(state)\n",
    "\n",
    "    # Ensure action is properly shaped (1D array, not scalar)\n",
    "    assert action.shape == (action_dim,), f\"Expected shape ({action_dim},) but got {action.shape}\"\n",
    "\n",
    "    for _ in range(100):\n",
    "        state = np.random.randn(state_dim)\n",
    "        action = agent.select_action(state)\n",
    "\n",
    "        # Ensure action is properly formatted\n",
    "        action = np.atleast_1d(action)  # Convert scalar to 1D array if needed\n",
    "\n",
    "        reward = np.random.randn()\n",
    "        next_state = np.random.randn(state_dim)\n",
    "        done = False\n",
    "\n",
    "        agent.replay_buffer.push(state, action, reward, next_state, done)\n",
    "        agent.train()\n",
    "\n",
    "    print(\"All tests passed!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_ddpg()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GENRATE RANDOM INPUT AND OTHER FUNCTION FOR ENV**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def generate_input():\n",
    "    cars_in_lane = [random.randint(0, 30) for _ in range(4)]  # 4 values from 0-100\n",
    "    wait_times = [random.randint(1, 60) for _ in range(4)]  # 4 values from 0-8\n",
    "    wait_times[-1]=0\n",
    "\n",
    "    return cars_in_lane + wait_times\n",
    "\n",
    "# Example usage\n",
    "random_input = generate_input()\n",
    "print(random_input)  # Output: [num1, num2, num3, num4, wt1, wt2, wt3, wt4,prelane]\n",
    "\n",
    "\n",
    "def rotate_list(lst, n):\n",
    "    return lst[n:] + lst[:n]  # Moves first `n` elements to the end\n",
    "\n",
    "\n",
    "\n",
    "print(rotate_list([1,2,3,4],1))\n",
    "\n",
    "\n",
    "import random\n",
    "\n",
    "def rotate_update(wait_times, nn_output):\n",
    "\n",
    "    selected_lane = 0 # Extract lane number (0-3)\n",
    "    green_time = nn_output[0]  # Extract green light duration (in seconds)\n",
    "\n",
    "    # Split the wait_times list\n",
    "    carsinlane = wait_times[:4]  # First 4 elements: Number of cars in each lane\n",
    "    carswaittime = wait_times[4:8]  # Last 4 elements: Waiting time per lane\n",
    "\n",
    "    # 🔹 Randomized car flow calculation\n",
    "    cars_per_second = 1\n",
    "    cars_passed = min(carsinlane[selected_lane], cars_per_second * green_time)\n",
    "\n",
    "    # 🔹 Update selected lane cars\n",
    "    carsinlane[selected_lane] -= cars_passed\n",
    "\n",
    "    # 🔹 Update waiting time for selected lane\n",
    "    carswaittime[selected_lane] = 0\n",
    "\n",
    "    # 🔹 Update waiting time for other lanes\n",
    "    for i in range(4):\n",
    "        if i != selected_lane:\n",
    "            if carsinlane[i] > 0:\n",
    "                carswaittime[i] += green_time  # Increase wait time for lanes with cars\n",
    "            else:\n",
    "                carswaittime[i] = 0  # If no cars, reset wait time\n",
    "    selected_lane=[selected_lane]\n",
    "   # print(carsinlane)\n",
    "    carsinlane=rotate_list(carsinlane, 1)\n",
    "   # print(carsinlane)\n",
    "   # print(carswaittime)\n",
    "    carswaittime=rotate_list(carswaittime, 1)\n",
    "   # print(carswaittime)\n",
    "\n",
    "\n",
    "    return carsinlane + carswaittime\n",
    "\n",
    "# 🔹 Example usage\n",
    "wait_times =  generate_input()  # Initial cars & wait times\n",
    "print(wait_times)\n",
    "nn_output = [10]  # Lane 2 gets green light for 10 sec\n",
    "\n",
    "updated_wait_times = rotate_update(wait_times, nn_output)\n",
    "print(updated_wait_times)  # Updated list with cars reduced properly\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**REWARD FUNCTION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "\n",
    "def calculate_reward(input_list, output_list, nn_output):\n",
    "    \"\"\"\n",
    "    Reward function for reinforcement learning in traffic management.\n",
    "\n",
    "    Parameters:\n",
    "        input_list: List containing initial traffic state [cars_in_lane, wait_times].\n",
    "        output_list: List containing updated traffic state after signal change.\n",
    "        nn_output: Output from the neural network [allocated green time].\n",
    "\n",
    "    Returns:\n",
    "        reward: A numerical value guiding reinforcement learning optimization.\n",
    "    \"\"\"\n",
    "\n",
    "    # **1. Extract input and output values**\n",
    "    cars_in_lane = input_list[:4]       # Cars currently in each lane\n",
    "    new_cars_in_lane = output_list[:4]  # Updated number of cars after signal change\n",
    "    cars_wait_time = input_list[4:8]    # Waiting times before signal change\n",
    "    new_cars_wait_time = output_list[4:8]  # Updated waiting times after signal change\n",
    "\n",
    "    # **2. Identify lanes exceeding max wait time (120 sec)**\n",
    "    exceeded_times = [wait for wait in new_cars_wait_time if wait >= 120]\n",
    "    diff_exceeded = [wait - 120 for wait in new_cars_wait_time if wait >= 120]\n",
    "\n",
    "    # Number of lanes exceeding max wait\n",
    "    num_exceeded = len(exceeded_times)\n",
    "\n",
    "    # **3. Extract neural network outputs**\n",
    "    selected_lane = 0  # Always the first lane\n",
    "    given_green = nn_output[0]  # Green time assigned\n",
    "\n",
    "    # **4. Calculate number of cars that passed during this cycle**\n",
    "    cars_passed = cars_in_lane[selected_lane] - new_cars_in_lane[-1]\n",
    "\n",
    "    # **5. Initialize reward**\n",
    "    reward = 0\n",
    "\n",
    "    # **6. Penalize exceeding 120 sec wait**\n",
    "    reward -= sum(diff_exceeded)\n",
    "    if num_exceeded > 0:\n",
    "        reward *= num_exceeded  # Amplify penalty if multiple lanes exceed\n",
    "\n",
    "    # **7. Encourage passing cars but balance time distribution**\n",
    "    reward += cars_passed\n",
    "\n",
    "    # **8. Prevent over-assigning time to a single lane**\n",
    "    avg_cars = statistics.mean(cars_in_lane)\n",
    "    if cars_in_lane[selected_lane] < avg_cars * 0.5 and given_green > 10:\n",
    "        reward -= 5\n",
    "\n",
    "    # **9. Encourage efficiency**\n",
    "    # If no cars remain in selected lane after green, give a small bonus\n",
    "    if new_cars_in_lane[-1] == 0:\n",
    "        reward += 5\n",
    "\n",
    "    return reward\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TRAINING MODEL AND TESTING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_ddpg():\n",
    "    state_dim = 8\n",
    "    action_dim = 1\n",
    "    agent = DDPG(state_dim, action_dim)\n",
    "\n",
    "    # 🔹 Training Phase\n",
    "    print(\"Training the model...\")\n",
    "    state = np.array(generate_input()).reshape(state_dim)  # Initial state\n",
    "\n",
    "    for _ in range(1):  # Training loop\n",
    "        while True:\n",
    "            action = np.atleast_1d(agent.select_action(state))\n",
    "\n",
    "            nstate = state.tolist()\n",
    "            naction = action.tolist()\n",
    "\n",
    "            temp_next_state = rotate_update(nstate, naction)  # Simulate environment\n",
    "            next_state = np.array(temp_next_state).flatten()  # Convert to array\n",
    "\n",
    "            reward = calculate_reward(next_state, state, action)\n",
    "            done = all(x == 0 for x in temp_next_state[:4])  # Stop when all cars clear\n",
    "\n",
    "            agent.replay_buffer.push(state, action, reward, next_state, done)\n",
    "            agent.train()\n",
    "\n",
    "            state = next_state  # Continue with new state\n",
    "\n",
    "            if done:\n",
    "                break  # Exit loop if all cars are cleared\n",
    "\n",
    "        state = np.array(generate_input()).reshape(state_dim)  # Generate new input\n",
    "\n",
    "    print(\"Training completed!\")\n",
    "\n",
    "# Save model\n",
    "    agent.save_model()\n",
    "    print(\"model saved!!!\")\n",
    "\n",
    "    # 🔹 Testing Phase (Using Trained Model)\n",
    "    print(\"Testing the trained model...\")\n",
    "    state = np.array(generate_input()).reshape(state_dim)  # Initial test state\n",
    "\n",
    "    while True:\n",
    "        action = np.atleast_1d(agent.select_action(state))  # Model selects action\n",
    "\n",
    "        nstate = state.tolist()\n",
    "        naction = action.tolist()\n",
    "\n",
    "        temp_next_state = rotate_update(nstate, naction)  # Simulate environment\n",
    "        next_state = np.array(temp_next_state).flatten()  # Convert to array\n",
    "\n",
    "        print(f\"State: {state.tolist()}, Action: {action.tolist()}, Next State: {next_state.tolist()}\")\n",
    "\n",
    "        done = all(x == 0 for x in temp_next_state[:4])  # Stop when all cars are cleared\n",
    "\n",
    "        state = next_state  # Continue with updated state\n",
    "\n",
    "        if done:\n",
    "            print(\"Testing completed! All cars cleared.\")\n",
    "            break  # Exit loop\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_ddpg()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
